# ğŸ¦™ Ollama Chat Playground

A monorepo experiment for running an AI chatbot locally using [Ollama](https://ollama.com/) + [LLaMA 3](https://ollama.com/library/llama3), with a Next.js frontend and Express backend â€” no external APIs required.

---

## ğŸ§± Tech Stack

- **Frontend**: Next.js (App Router), TailwindCSS, React
- **Backend**: Express.js (TypeScript)
- **Local LLM**: Ollama running LLaMA 3
- **Monorepo**: Managed with [pnpm workspaces](https://pnpm.io/workspaces)

---

## ğŸ“ Project Structure

```
ollama-chat-playground/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/       # Next.js frontend (AmberSky UI)
â”‚   â””â”€â”€ server/    # Express API backend
â”œâ”€â”€ package.json   # Root workspace configuration
â”œâ”€â”€ pnpm-workspace.yaml
```

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/your-username/ollama-chat-playground.git
cd ollama-chat-playground
```

### 2. Install dependencies

```bash
pnpm install
```

### 3. Start Ollama locally (if not running already)

```bash
ollama run llama3
```

### 4. Start the monorepo (frontend + backend)

```bash
pnpm dev
```

- Frontend: [http://localhost:3000](http://localhost:3000)
- Backend API: [http://localhost:3001/api/chat](http://localhost:3001/api/chat)

---

## ğŸ§  How It Works

- The frontend sends a request to `/api/chat`
- This routes to the Express backend at `localhost:3001`
- The backend talks to the local Ollama instance and returns a response

---

## ğŸ›  Scripts

From the root:

```bash
pnpm dev          # Run both web and server concurrently
pnpm dev:web      # Run frontend only
pnpm dev:server   # Run backend only
```

---

## ğŸ“¦ Environment Variables

If needed, create `.env` files inside `apps/web` and `apps/server`.  
Right now, the API URLs are hardcoded for local development.

---

## ğŸ’¬ Chat Example

In the AmberSky interface, click the ğŸ’¬ button to open a modal and start chatting with AmberAI, powered by LLaMA 3 running on your machine.

---

## ğŸ“„ License

MIT
